{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e1d46a-1a4d-4b1c-b168-e2d9ee166ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models, layers, datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc52f71f-29ab-4c62-a7ae-7ae0a1db4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a24f1ce7-a2ce-45d3-bc85-05e8fde21eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed path: c:\\Users\\Fear2\\OneDrive - University of North Georgia\\FALL 2023\\Cyber capstone\\Experimentation\\SortedData\\train\\clubs\\ace of clubs\n",
      "Does the path exist? False\n"
     ]
    }
   ],
   "source": [
    "# CHANGE USERNAME WHEN WORKING BETWEEN LAPTOP AND DESKTOP!!\n",
    "import os\n",
    "base_dir = \"c:\\\\Users\\\\Fear2\\\\OneDrive - University of North Georgia\\\\FALL 2023\\\\Cyber capstone\\\\Experimentation\\\\SortedData\"\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'valid')\n",
    "# Replace with your actual base directory path\n",
    "\n",
    "# Replace with the actual subfolder path you want to check\n",
    "test_path = os.path.join(base_dir, \"train\", \"clubs\", \"ace of clubs\")\n",
    "\n",
    "print(\"Constructed path:\", test_path)\n",
    "print(\"Does the path exist?\", os.path.isdir(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ca28d1-bf1f-49bd-9b48-8c3bed0c8c32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking suit directory: c:\\Users\\Fear2\\OneDrive - University of North Georgia\\FALL 2023\\Cyber capstone\\Experimentation\\SortedData\\train\\hearts\n",
      "Checking suit directory: c:\\Users\\Fear2\\OneDrive - University of North Georgia\\FALL 2023\\Cyber capstone\\Experimentation\\SortedData\\train\\diamonds\n",
      "Checking suit directory: c:\\Users\\Fear2\\OneDrive - University of North Georgia\\FALL 2023\\Cyber capstone\\Experimentation\\SortedData\\train\\clubs\n",
      "Checking suit directory: c:\\Users\\Fear2\\OneDrive - University of North Georgia\\FALL 2023\\Cyber capstone\\Experimentation\\SortedData\\train\\spades\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m test_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m train_images, train_labels, train_class_labels \u001b[38;5;241m=\u001b[39m load_dataset(train_dir)\n\u001b[0;32m     54\u001b[0m valid_images, valid_labels, valid_class_labels \u001b[38;5;241m=\u001b[39m load_dataset(validation_dir)\n\u001b[0;32m     55\u001b[0m test_images, test_labels, test_class_labels \u001b[38;5;241m=\u001b[39m load_dataset(test_dir)\n",
      "Cell \u001b[1;32mIn[5], line 36\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(base_dir, target_size)\u001b[0m\n\u001b[0;32m     34\u001b[0m combined \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(images, labels))\n\u001b[0;32m     35\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(combined)\n\u001b[1;32m---> 36\u001b[0m images[:], labels[:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mcombined)                    \n\u001b[0;32m     38\u001b[0m images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(images, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize to [0, 1]\u001b[39;00m\n\u001b[0;32m     39\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(labels, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def load_dataset(base_dir, target_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_labels = {'hearts': 0, 'diamonds': 1, 'clubs': 2, 'spades': 3}\n",
    "    \n",
    "    # Navigate through each suit folder\n",
    "    for suit in class_labels.keys():\n",
    "        suit_dir = os.path.join(base_dir, suit)\n",
    "        print(f\"Checking suit directory: {suit_dir}\")  # Debug print\n",
    "        if os.path.isdir(suit_dir):\n",
    "            label_index = class_labels[suit]\n",
    "            print(f\"Label index for {suit}: {label_index}\")  # Debug print\n",
    "\n",
    "            # Navigate through each card folder within the suit folder\n",
    "            for card_folder in os.listdir(suit_dir):\n",
    "                card_folder_path = os.path.join(suit_dir, card_folder)\n",
    "                #print(f\"Checking card folder: {card_folder_path}\")  # Add this debug print\n",
    "                if os.path.isdir(card_folder_path):\n",
    "                    # Load each image from the card folder\n",
    "                    for image_file in os.listdir(card_folder_path):\n",
    "                        image_path = os.path.join(card_folder_path, image_file)\n",
    "                        image = load_img(image_path, target_size=target_size)\n",
    "                        #print(f\"Loading image: {image_path}\")  # Debug print\n",
    "                        image_array = img_to_array(image)\n",
    "                        images.append(image_array)\n",
    "                        labels.append(label_index)\n",
    "    \n",
    "    # After loading all images and labels, shuffle them\n",
    "    combined = list(zip(images, labels))\n",
    "    np.random.shuffle(combined)\n",
    "    images[:], labels[:] = zip(*combined)                    \n",
    "    \n",
    "    images = np.array(images, dtype='float32') / 255.0  # Normalize to [0, 1]\n",
    "    labels = np.array(labels, dtype='int32')\n",
    "    \n",
    "    # Print some of the labels to verify they are correct\n",
    "    print(labels[:100])\n",
    "    \n",
    "    return images, labels, class_labels\n",
    "\n",
    "# Directory Definitions\n",
    "base_dir = \"c:\\\\Users\\\\Fear2\\\\OneDrive - University of North Georgia\\\\FALL 2023\\\\Cyber capstone\\\\Experimentation\\\\SortedData\"\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'valid')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Usage\n",
    "train_images, train_labels, train_class_labels = load_dataset(train_dir)\n",
    "valid_images, valid_labels, valid_class_labels = load_dataset(validation_dir)\n",
    "test_images, test_labels, test_class_labels = load_dataset(test_dir)\n",
    "\n",
    "# Check shapes and class labels\n",
    "print(train_images.shape, train_labels.shape)\n",
    "print(valid_images.shape, valid_labels.shape)\n",
    "print(test_images.shape, test_labels.shape)\n",
    "print(train_class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3476ec02-c1a5-4c58-a676-0bb948f2fd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7509, 224, 224, 3) (7509,)\n",
      "(260, 224, 224, 3) (260,)\n",
      "(260, 224, 224, 3) (260,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the arrays\n",
    "print(train_images.shape, train_labels.shape)\n",
    "print(test_images.shape, test_labels.shape)\n",
    "print(valid_images.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d35c4d6-6c9d-4c83-930e-60426b9d5847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the training dataset\n",
    "train_indices = np.arange(train_images.shape[0])\n",
    "np.random.shuffle(train_indices)\n",
    "train_images = train_images[train_indices]\n",
    "train_labels = train_labels[train_indices]\n",
    "\n",
    "# Shuffle the validation dataset\n",
    "valid_indices = np.arange(valid_images.shape[0])\n",
    "np.random.shuffle(valid_indices)\n",
    "valid_images = valid_images[valid_indices]\n",
    "valid_labels = valid_labels[valid_indices]\n",
    "\n",
    "# Optionally, if you also want to shuffle the test dataset:\n",
    "test_indices = np.arange(test_images.shape[0])\n",
    "np.random.shuffle(test_indices)\n",
    "test_images = test_images[test_indices]\n",
    "test_labels = test_labels[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "105e31de-1869-44e9-8cd9-676da01d28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Data augmentation\n",
    "data_gen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "#checkpoint_cb = ModelCheckpoint(\"best_model.h5\", save_best_only=True)\n",
    "#early_stopping_cb = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "#reduce_lr_cb = ReduceLROnPlateau(factor=0.2, patience=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6a75d83-8474-4c22-b617-649da5d83ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 224, 224, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 112, 112, 32)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 112, 112, 32)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 56, 56, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 56, 56, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 56, 56, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 28, 28, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 28, 28, 128)       0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 100352)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               12845184  \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12938948 (49.36 MB)\n",
      "Trainable params: 12938948 (49.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b016202d-bc2e-4e88-9bb9-50ebbcb7bf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 171s 723ms/step - loss: 1.3994 - accuracy: 0.3554 - val_loss: 1.0612 - val_accuracy: 0.4654\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 176s 749ms/step - loss: 1.1190 - accuracy: 0.4432 - val_loss: 0.9447 - val_accuracy: 0.5308\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 176s 749ms/step - loss: 1.0150 - accuracy: 0.5014 - val_loss: 0.7681 - val_accuracy: 0.6462\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 177s 754ms/step - loss: 0.9041 - accuracy: 0.5691 - val_loss: 0.6646 - val_accuracy: 0.6769\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 162s 688ms/step - loss: 0.8295 - accuracy: 0.5916 - val_loss: 0.5573 - val_accuracy: 0.6962\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 161s 685ms/step - loss: 0.7586 - accuracy: 0.6374 - val_loss: 0.5123 - val_accuracy: 0.7731\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 162s 686ms/step - loss: 0.7435 - accuracy: 0.6448 - val_loss: 0.4964 - val_accuracy: 0.7423\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 164s 697ms/step - loss: 0.7141 - accuracy: 0.6591 - val_loss: 0.4467 - val_accuracy: 0.8000\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 174s 740ms/step - loss: 0.6895 - accuracy: 0.6700 - val_loss: 0.4459 - val_accuracy: 0.8231\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 176s 748ms/step - loss: 0.6706 - accuracy: 0.6772 - val_loss: 0.4480 - val_accuracy: 0.8115\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 174s 740ms/step - loss: 0.6574 - accuracy: 0.6842 - val_loss: 0.4100 - val_accuracy: 0.8231\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 166s 704ms/step - loss: 0.6249 - accuracy: 0.7002 - val_loss: 0.4109 - val_accuracy: 0.8346\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 161s 685ms/step - loss: 0.6173 - accuracy: 0.7122 - val_loss: 0.3369 - val_accuracy: 0.8731\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 163s 693ms/step - loss: 0.5921 - accuracy: 0.7317 - val_loss: 0.2835 - val_accuracy: 0.8923\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 165s 699ms/step - loss: 0.5658 - accuracy: 0.7503 - val_loss: 0.2713 - val_accuracy: 0.8885\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 171s 725ms/step - loss: 0.5314 - accuracy: 0.7777 - val_loss: 0.2256 - val_accuracy: 0.9231\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 170s 723ms/step - loss: 0.5106 - accuracy: 0.7915 - val_loss: 0.1819 - val_accuracy: 0.9462\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 171s 725ms/step - loss: 0.4860 - accuracy: 0.8033 - val_loss: 0.1928 - val_accuracy: 0.9269\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 171s 727ms/step - loss: 0.4520 - accuracy: 0.8170 - val_loss: 0.1509 - val_accuracy: 0.9423\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 171s 726ms/step - loss: 0.4348 - accuracy: 0.8309 - val_loss: 0.1463 - val_accuracy: 0.9500\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 171s 727ms/step - loss: 0.4152 - accuracy: 0.8387 - val_loss: 0.1384 - val_accuracy: 0.9538\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 170s 723ms/step - loss: 0.4004 - accuracy: 0.8451 - val_loss: 0.1224 - val_accuracy: 0.9577\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 169s 719ms/step - loss: 0.4093 - accuracy: 0.8487 - val_loss: 0.1403 - val_accuracy: 0.9577\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 170s 722ms/step - loss: 0.3800 - accuracy: 0.8530 - val_loss: 0.1214 - val_accuracy: 0.9731\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 167s 712ms/step - loss: 0.3749 - accuracy: 0.8548 - val_loss: 0.1403 - val_accuracy: 0.9654\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 160s 679ms/step - loss: 0.3851 - accuracy: 0.8558 - val_loss: 0.1190 - val_accuracy: 0.9615\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 160s 678ms/step - loss: 0.3663 - accuracy: 0.8634 - val_loss: 0.1192 - val_accuracy: 0.9615\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 158s 673ms/step - loss: 0.3450 - accuracy: 0.8692 - val_loss: 0.1243 - val_accuracy: 0.9654\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 158s 671ms/step - loss: 0.3512 - accuracy: 0.8696 - val_loss: 0.1117 - val_accuracy: 0.9654\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 169s 719ms/step - loss: 0.3381 - accuracy: 0.8712 - val_loss: 0.1114 - val_accuracy: 0.9654\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 178s 757ms/step - loss: 0.3220 - accuracy: 0.8780 - val_loss: 0.1317 - val_accuracy: 0.9615\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 178s 758ms/step - loss: 0.3449 - accuracy: 0.8712 - val_loss: 0.0873 - val_accuracy: 0.9846\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 169s 719ms/step - loss: 0.3189 - accuracy: 0.8845 - val_loss: 0.0811 - val_accuracy: 0.9885\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 172s 730ms/step - loss: 0.3109 - accuracy: 0.8828 - val_loss: 0.0738 - val_accuracy: 0.9885\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 179s 762ms/step - loss: 0.2968 - accuracy: 0.8880 - val_loss: 0.0758 - val_accuracy: 0.9885\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 158s 672ms/step - loss: 0.3092 - accuracy: 0.8887 - val_loss: 0.0803 - val_accuracy: 0.9846\n",
      "Epoch 37/50\n",
      "106/235 [============>.................] - ETA: 1:24 - loss: 0.3055 - accuracy: 0.8812"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the model with a generator\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# More epochs\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalid_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the model with a generator\n",
    "history = model.fit(\n",
    "    data_gen.flow(train_images, train_labels, batch_size=32),\n",
    "    epochs=50,  # More epochs\n",
    "    validation_data=(valid_images, valid_labels)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "090c1657-dec0-4928-a84f-9fec1c010853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fear2\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('PlayingCard.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f76459-dcab-48bc-b405-fe909624c420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "model = load_model('PlayingCard.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91aee7dd-6c24-411e-8f1f-7ae7665702ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 92ms/step - loss: 0.1106 - accuracy: 0.9577\n",
      "Test Loss: 0.1106150671839714\n",
      "Test Accuracy: 0.9576923251152039\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "699a8aa3-d0f5-4359-938c-971463849eca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(valid_images)\n\u001b[0;32m      8\u001b[0m predicted_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Calculate the confusion matrix\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_images' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "predictions = model.predict(valid_images)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(valid_labels, predicted_classes)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=train_class_labels, yticklabels=train_class_labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(valid_images, valid_labels)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a5c97-81cd-4e36-82d4-b260f686b8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
