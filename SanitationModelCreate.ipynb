{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be2f3e4-5e3b-4722-bcc9-a61c2c6840fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models, layers, datasets\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.estimators.classification import KerasClassifier\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eabdc9c-f30a-4aa2-ae8b-e1cc1afe60bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking suit directory: c:\\Users\\alec\\OneDrive - University of North Georgia\\FALL 2023\\Cyber capstone\\Experimentation\\SortedData\\train\\hearts\n",
      "Label index for hearts: 0\n",
      "Checking suit directory: c:\\Users\\alec\\OneDrive - University of North Georgia\\FALL 2023\\Cyber capstone\\Experimentation\\SortedData\\train\\diamonds\n",
      "Label index for diamonds: 1\n",
      "Checking suit directory: c:\\Users\\alec\\OneDrive - University of North Georgia\\FALL 2023\\Cyber capstone\\Experimentation\\SortedData\\train\\clubs\n",
      "Label index for clubs: 2\n",
      "Checking suit directory: c:\\Users\\alec\\OneDrive - University of North Georgia\\FALL 2023\\Cyber capstone\\Experimentation\\SortedData\\train\\spades\n",
      "Label index for spades: 3\n",
      "[3 2 0 2 1 0 0 1 0 1 3 3 1 1 3 3 3 0 1 1 0 2 2 3 2 0 3 3 3 3 1 0 0 1 3 2 1\n",
      " 0 3 3 3 0 2 3 3 1 1 0 1 0 2 1 2 0 1 2 1 1 1 3 3 0 2 1 2 1 3 3 2 1 3 1 3 1\n",
      " 3 0 1 0 1 3 1 1 3 2 2 3 2 3 2 3 3 0 2 2 0 3 1 3 2 1]\n",
      "<class 'numpy.ndarray'> (7509, 224, 224, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alec\\miniconda3\\envs\\jupyterenv\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8509, 224, 224, 3) (8509,)\n"
     ]
    }
   ],
   "source": [
    "model=load_model('PlayingCard.h5')\n",
    "\n",
    "# Wrap the model with ART's KerasClassifier\n",
    "classifier = KerasClassifier(model=model, clip_values=(0, 1), use_logits=False)\n",
    "\n",
    "# Initialize the attack method\n",
    "attack = FastGradientMethod(estimator=classifier, eps=0.1)\n",
    "\n",
    "num_poisoned = 1000  \n",
    "\n",
    "\n",
    "def generate_poisoned_images(clean_images, num_poisoned, target_size=(224, 224)):\n",
    "    # Select a random subset of clean images to poison\n",
    "    indices = np.random.choice(range(clean_images.shape[0]), size=num_poisoned, replace=False)\n",
    "    sample_images = clean_images[indices]\n",
    "    \n",
    "    # Generate adversarial (poisoned) examples from the sample\n",
    "    poisoned_images = attack.generate(x=sample_images)\n",
    "    \n",
    "    return poisoned_images\n",
    "\n",
    "def load_dataset_with_poison(base_dir, target_size=(224, 224), num_poisoned=1000):\n",
    "    # Load clean images using the existing load_dataset function\n",
    "    clean_images, clean_labels, class_labels = load_dataset(base_dir, target_size)\n",
    "    print(type(clean_images), clean_images.shape)\n",
    "    \n",
    "    # Fix labels from 4 classes to all zeros\n",
    "    clean_labels = np.zeros(clean_images.shape[0])\n",
    "    \n",
    "    # Generate poisoned images\n",
    "    poisoned_images = generate_poisoned_images(clean_images, num_poisoned, target_size)\n",
    "    poisoned_labels = np.ones(num_poisoned)  # Label poisoned images as 1\n",
    "    \n",
    "    # Combine the clean and poisoned images and labels\n",
    "    combined_images = np.concatenate([clean_images, poisoned_images], axis=0)\n",
    "    combined_labels = np.concatenate([clean_labels, poisoned_labels], axis=0)\n",
    "    \n",
    "    # Shuffle the combined dataset\n",
    "    indices = np.arange(combined_images.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    combined_images = combined_images[indices]\n",
    "    combined_labels = combined_labels[indices]\n",
    "    \n",
    "    return combined_images, combined_labels\n",
    "\n",
    "def load_dataset(base_dir, target_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_labels = {'hearts': 0, 'diamonds': 1, 'clubs': 2, 'spades': 3}\n",
    "    \n",
    "    # Navigate through each suit folder\n",
    "    for suit in class_labels.keys():\n",
    "        suit_dir = os.path.join(base_dir, suit)\n",
    "        print(f\"Checking suit directory: {suit_dir}\")  # Debug print\n",
    "        if os.path.isdir(suit_dir):\n",
    "            label_index = class_labels[suit]\n",
    "            print(f\"Label index for {suit}: {label_index}\")  # Debug print\n",
    "\n",
    "            # Navigate through each card folder within the suit folder\n",
    "            for card_folder in os.listdir(suit_dir):\n",
    "                card_folder_path = os.path.join(suit_dir, card_folder)\n",
    "                #print(f\"Checking card folder: {card_folder_path}\")  # Add this debug print\n",
    "                if os.path.isdir(card_folder_path):\n",
    "                    # Load each image from the card folder\n",
    "                    for image_file in os.listdir(card_folder_path):\n",
    "                        image_path = os.path.join(card_folder_path, image_file)\n",
    "                        image = load_img(image_path, target_size=target_size)\n",
    "                        #print(f\"Loading image: {image_path}\")  # Debug print\n",
    "                        image_array = img_to_array(image)\n",
    "                        images.append(image_array)\n",
    "                        labels.append(label_index)\n",
    "    \n",
    "    # After loading all images and labels, shuffle them\n",
    "    combined = list(zip(images, labels))\n",
    "    np.random.shuffle(combined)\n",
    "    images[:], labels[:] = zip(*combined)                    \n",
    "    \n",
    "    images = np.array(images, dtype='float32') / 255.0  # Normalize to [0, 1]\n",
    "    labels = np.array(labels, dtype='int32')\n",
    "    \n",
    "    # Print some of the labels to verify they are correct\n",
    "    print(labels[:100])\n",
    "    \n",
    "    return images, labels, class_labels\n",
    "\n",
    "# Load your clean dataset\n",
    "base_dir = \"c:\\\\Users\\\\alec\\\\OneDrive - University of North Georgia\\\\FALL 2023\\\\Cyber capstone\\\\Experimentation\\\\SortedData\"\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "\n",
    "# Now use the updated function to load your dataset with poisoned images\n",
    "train_images_with_poison, train_labels_with_poison = load_dataset_with_poison(train_dir)\n",
    "\n",
    "# Since this is a binary classification task now, you will have just two classes: 0 for clean, 1 for poisoned\n",
    "binary_class_labels = {'clean': 0, 'poisoned': 1}\n",
    "\n",
    "# Check shapes and labels\n",
    "print(train_images_with_poison.shape, train_labels_with_poison.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b14cac-9393-4d01-acba-98a9106338be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split training and validation data sets 80% and 20%\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images_with_poison, train_labels_with_poison, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c71e618-c3ae-409e-800f-65c7e66c7838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Clearing the TensorFlow backend\n",
    "K.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e36828-21d1-410b-9f9c-65d300469742",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e769df-1a8b-4483-abf4-ec5dffa2f937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 224, 224, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 112, 112, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 112, 112, 32)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 56, 56, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 56, 56, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 200704)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               25690240  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25709761 (98.07 MB)\n",
      "Trainable params: 25709761 (98.07 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Train on 6807 samples, validate on 1702 samples\n",
      "Epoch 1/10\n",
      "6807/6807 [==============================] - ETA: 0s - loss: 0.6975 - accuracy: 0.8779"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alec\\miniconda3\\envs\\jupyterenv\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6807/6807 [==============================] - 127s 19ms/sample - loss: 0.6975 - accuracy: 0.8779 - val_loss: 0.4584 - val_accuracy: 0.8796\n",
      "Epoch 2/10\n",
      "6807/6807 [==============================] - 127s 19ms/sample - loss: 0.2804 - accuracy: 0.8842 - val_loss: 0.3103 - val_accuracy: 0.8796\n",
      "Epoch 3/10\n",
      "6807/6807 [==============================] - 130s 19ms/sample - loss: 0.1965 - accuracy: 0.9098 - val_loss: 0.1725 - val_accuracy: 0.9260\n",
      "Epoch 4/10\n",
      "6807/6807 [==============================] - 128s 19ms/sample - loss: 0.1592 - accuracy: 0.9282 - val_loss: 0.1881 - val_accuracy: 0.9213\n",
      "Epoch 5/10\n",
      "6807/6807 [==============================] - 127s 19ms/sample - loss: 0.1276 - accuracy: 0.9468 - val_loss: 0.1202 - val_accuracy: 0.9571\n",
      "Epoch 6/10\n",
      "6807/6807 [==============================] - 127s 19ms/sample - loss: 0.1049 - accuracy: 0.9533 - val_loss: 0.1359 - val_accuracy: 0.9401\n",
      "Epoch 7/10\n",
      "6807/6807 [==============================] - 127s 19ms/sample - loss: 0.0774 - accuracy: 0.9690 - val_loss: 0.0627 - val_accuracy: 0.9788\n",
      "Epoch 8/10\n",
      "6807/6807 [==============================] - 127s 19ms/sample - loss: 0.0818 - accuracy: 0.9703 - val_loss: 0.1046 - val_accuracy: 0.9565\n",
      "Epoch 9/10\n",
      "6807/6807 [==============================] - 127s 19ms/sample - loss: 0.0687 - accuracy: 0.9755 - val_loss: 0.0848 - val_accuracy: 0.9665\n",
      "Epoch 10/10\n",
      "6807/6807 [==============================] - 128s 19ms/sample - loss: 0.0528 - accuracy: 0.9780 - val_loss: 0.0838 - val_accuracy: 0.9736\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "input_shape = (224, 224, 3)  # Images are 224x224 RGB images\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Single neuron for binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # Suitable for binary classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    epochs=10, \n",
    "                    batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64288590-84da-44de-9cf8-2f57981a5270",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alec\\miniconda3\\envs\\jupyterenv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('PlayingCardSanitation.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66c909-bc18-481e-89de-6a36f16a0ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
